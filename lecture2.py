# -*- coding: utf-8 -*-
"""lecture2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s_Vu5JBLqHeHhG2Bzfd7WOm8rT0P07TV
"""

import nltk
import numpy as np
import matplotlib.pyplot as plt
# nltk.download('punkt')

from nltk.tokenize import word_tokenize, wordpunct_tokenize
from nltk.stem import PorterStemmer

"""# Curse of Dimensionality
$$e_{d}(r)= \frac{(rL)^{d}}{L^{d}}L=r^{d}L$$
"""

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 10))
L = 1
n = 100
r = np.linspace(0, 1, num=n)
N = np.linspace(100, 10**5, num =1000)

for d in [1, 5, 10, 20, 100, 1000]:
  e = r ** (1/d) * L
  axes[0].plot(r, e, label=f'd={d}')

  dist = (1 - 0.5**(1/N))**(1 /d)

  axes[1].plot(N, dist, label=f'd={d}')

axes[0].legend(fontsize=16)
axes[1].legend(fontsize=16)
axes[0].set_xlabel(f'r', fontsize=16)
axes[0].set_ylabel(f'Edge length', fontsize=16)

axes[1].set_xlabel(f'N', fontsize=16)
axes[1].set_ylabel(f'Median distance between origin and closest neighbhor', fontsize=16)

plt.show()

"""# Principal Component Analysis
We choose $X_{1}, X_{2}\sim \mathcal{N}(0, 1)$. Then, we add $X_{1}^{2}, X_{2}^{2}, X_{1}X_{2}, X_{1}+X_{2}, 2X_{1}, 2X_{2}, (X_{1}+X_{2})^{3}
X_{1}^{3}, X_{2}^{3}$
"""

n = 1000
d = 2
X = np.random.normal(size=(n, d))
# add quadratic term
expansion = []
for i in range(X.shape[1]):
      for j in range(i + 1):
          expansion.append(X[:, i] + X[:, j])
          expansion.append(X[:, i] * X[:, j])
          expansion.append((X[:, i] + X[:, j])**3)
add_terms = np.column_stack(expansion)
X2 = np.column_stack([X, add_terms])
print(X2.shape)
# SVD
u, s, v = np.linalg.svd(X2, compute_uv=True, full_matrices=False)
q = np.dot(u, X2.transpose())

# reconstruction
reconstruction_loss = np.zeros(s.shape[0])
for k in range(s.shape[0]):
  xhat = np.dot(np.dot(u[:, :k], np.diag(s[:k])), v[:k, :])
  reconstruction_loss[k] = np.sum((xhat - X2) ** 2, 1).mean(0)


fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 10))
axes[0].bar(np.arange(s.shape[0]), s)
axes[0].set_title('Singular Values', fontsize=16)
axes[0].set_xlabel('Component', fontsize=16)
axes[0].set_ylabel('Singular Values', fontsize=16)

axes[1].plot(np.arange(s.shape[0]), reconstruction_loss)
axes[1].set_title('Reconstruction Loss', fontsize=16)
axes[1].set_xlabel('Number of Components', fontsize=16)
axes[1].set_ylabel('Reconstruction Loss', fontsize=16)

# zoom in
x1, x2, y1, y2 = 6, 10, -5, 10
axins = axes[1].inset_axes([0.5, 0.5, 0.47, 0.47])
axins.plot(np.arange(s.shape[0]), reconstruction_loss)
axins.set_xlim(x1, x2)
axins.set_ylim(y1, y2)

axes[1].indicate_inset_zoom(axins, edgecolor="black")

plt.show()

"""# Tokenization: 

"""

doc = "It's not straight-forward to perform so-called tokenization"

# word_tokenize
tokens_word = word_tokenize(doc)

# punctuation tokenize
tokens_punc = wordpunct_tokenize(doc)

print(tokens_word)
print(tokens_punc)

"""# Stemming"""

ps = PorterStemmer()

doc1 = 'Text mining is to identify useful information'
doc2 = 'Useful information is mined from text'
doc3 = ' Apple is delicious'

words_dict = set()

documents  = [doc1, doc2, doc3]
for d in documents:
  tokens = word_tokenize(d)


  for w in tokens:
    stem = ps.stem(w)
    words_dict.add(stem)

print(words_dict)



